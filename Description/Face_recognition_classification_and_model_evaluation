# Next we are gonna do classification, model fine tuning and evaluation 
* I prepare train and test sets using my model and i gonna train couple models 
* I start from SVC model and i reach very high accuracy
* And i plot my model's predictions on confusion matrix
* I also check classification report and reach average f1-score 0.92, which is very good

# Then i gonna check couple other models
* I put them into the model list and train on my train set 
* Then take predictions and check the score results
* And i see that also as good as SVM are models: LogisticRegression and LinearDiscriminant

# I want to reach the highest score as possible, so i gonna use cross_val_score technique
* So i supply Kflod model, set data splits to 5, then shuffle the data
* Next i supply my models to the cross_val_score, next X and Y variables and KFold model
* Finally i can take mean score from all scores 
* I see that 3 model reach high accuracy: LinearDiscriminant, LogisticRegression and SVC 
* Then i check confusion_matrix on my LinearDiscriminant model, when i check average f1-score its 0.94 which is pretty high

# Next i gonna do further model tuning
* I use LeaveOneOut model, which leave only 1 sample for testing 
* So i feed in to the cross_val_score LogisticRegression, which achieve score: 0.97
* And LinearDiscriminant, which achieve score: 0.99
* So still LinearDiscriminat win with other models

# Then i gonna illustrate precision and recall curves with help of signature module and plt
* First i gonna binarize my 40 classes 
* When i check the target shape its 400 by 40 and first target is just row vector with 39 zeros and 1 in place of corect class num.

# Then i create new train and test set for multiclass classification
* I use PCA on given X_train and transform using PCA X_train and X_test
* To do multi class classification we are gonna use One VS Rest Classifier

# What its doing is following: its splitting a multi class dataset into multiple binary classification problems
* Fortunately we have only 40 classes, because thera are some issues with very large number of classes
* It's because for each class ther's need to be create one model
* And one vs rest is because 40 classes dataset can be devided into 40 binary classification datasets
* Por example 1 vs (2-40), then 2 vs (1-40) without 2 and on

# Then i call OneVsRestClassifier on my LinearDiscriminant
* And i feed X and y train to my model, then i evaluate my model with X_test
* Next to get scores i create few dictionaries to store them
* And next i create for loop in range of number of classes
* Inside loop i define precision and recall called from metrics
* And feed my y_test from first row to the i-th element and also y_score [:, i] - from first to i-th
* Thats how we define previous explanation
* Then i also define avg-precision and feed the same y_test and y_score from first to i-th element
* Next we want to take micro precision and recall, to do that we need to flatten our (120, 40) y_test_multicalss and make it contiguous
* So we call ravel to do this 
* And we also take micro-average precision, by just passing y_test and y_score and set average to micro
* We achieve score 0.95 over all classes
